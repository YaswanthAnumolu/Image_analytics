# analytics/ai_algorithm.py
import openai
from PIL import Image
import torch
from transformers import CLIPProcessor, CLIPModel
from openai.error import RateLimitError
import time
#from analytics.ai_algorithm import ai_generate_answer
##################################################
def openai_answer_question(question, image_description, max_retries=5):
    """Use OpenAI's API to generate an answer based on the image description."""
    openai.api_key = 'sk-JG1hlQcL2T42bLOT13rYjmLEMIk0atDqyGCziGSQybT3BlbkFJmbEKHoiUG-ZF_aEbJ_F_tC4C4qrEJZIEjQvHWs-O4A'

    retries = 0
    while retries < max_retries:
        try:
            response = openai.ChatCompletion.create(
                model="gpt-3",
                messages=[
                    {"role": "system", "content": f"The image is described as: {image_description}"},
                    {"role": "user", "content": question}
                ]
            )
            return response.choices[0].message["content"]
        except RateLimitError as e:
            retries += 1
            if retries < max_retries:
                # Exponential backoff - wait longer between retries
                wait_time = 2 ** retries
                print(f"Rate limit exceeded. Retrying in {wait_time} seconds...")
                time.sleep(wait_time)
            else:
                raise e  # If max retries reached, raise the error
##################################################
# Initialize CLIP model
model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

def process_image_with_clip(image):
    """Process the image with CLIP and return features."""
    img = Image.open(image)
    inputs = processor(images=img, return_tensors="pt")

    # Get image features using CLIP
    outputs = model.get_image_features(**inputs)

    # Here you can derive some description or embedding
    # For example purposes, we'll use a placeholder for the description
    return "This is a placeholder description generated by the image processing model."

def openai_answer_question(question, image_description):
    """Use OpenAI's API to generate an answer based on the image description."""
    openai.api_key = 'sk-JG1hlQcL2T42bLOT13rYjmLEMIk0atDqyGCziGSQybT3BlbkFJmbEKHoiUG-ZF_aEbJ_F_tC4C4qrEJZIEjQvHWs-O4A'  # Set your OpenAI API key here

    response = openai.ChatCompletion.create(
        model="gpt-4o-mini",  # E nsure you're using the right model
        messages=[
            
            {"role": "system", "content": f"The image is described as: {image_description}"},
            {"role": "user", "content": question}
        ]
    )

    return response.choices[0].message["content"]



# Optionally, you can define a higher-level function to encapsulate image processing and question answering
def ai_generate_answer(image, question):
    """Process the image and generate an answer to the question."""
    image_description = process_image_with_clip(image)
    answer = openai_answer_question(question, image_description)
    return answer

# analytics/ai_algorithm.py
#def ai_answer_question(question, image):
    # Implement your logic to process the question and image
    # This could involve calling an external API, using a model, etc.
    # Return the answer
    #return "This is a placeholder answer."
